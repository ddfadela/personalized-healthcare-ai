%-----------------------------------------------------------------------------------------------------------------------------
% \chapter{Machine Learning-Based Intrusion Detection System Approaches}
% This section will provide an overview of various approaches that have been developed for intrusion detection.
% Many works are being carried out in this context to find the best parameters and results for intrusion detection in various environments, and this survey will examine some of the most recent studies. The section will conclude with a summary table of the reviewed approaches.\\
% ============================== ARTICLE 1 =====================================
\section{Towards Realizing the Vision of Precision Medicine: AI-Based Prediction of Clinical Drug Response}
\textbf{Article Reference:} \cite{article_1}

\subsection*{Overview}
This study uses machine learning to predict patient response to the epilepsy drug brivaracetam using integrated clinical and genomic data. The resulting model demonstrated strong performance (AUC: 0.76 training, 0.75 validation) and identified specific biomarkers associated with poor response. The research underscores the potential of ML models to support precision medicine and optimize clinical trials by targeting likely responders. This study highlights the potential of AI to personalize treatment strategies in epilepsy by predicting drug response, a key aspect of personalized medicine.

\subsection*{Dataset}
\begin{itemize}
    \item \textbf{Discovery dataset:} 235 adult patients from a phase III clinical trial (NCT01261325).
    \item \textbf{External validation dataset:} 47 patients from an independent trial (NCT00490035).
\end{itemize}

\subsection*{Data Processing}
/
\subsection*{Approach}
Multiple ML models were evaluated: sparse multi-block PLS-DA, multimodal neural networks, elastic net, gradient-boosted decision trees (GBDT), and stacked classifiers. The best performance was achieved using a GBDT model integrating all data types. GBDT models are well-suited for handling the complex interactions between clinical and genetic features, which is crucial for personalized drug response prediction. However, the inherent complexity of GBDT models can make it challenging to interpret the specific contributions of individual features, a limitation that future explainable AI (XAI) techniques could address.

\subsection*{Results}
\begin{itemize}
    \item AUC (training): 0.76
    \item AUC (validation): 0.75
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item Addressing high dimensionality and sparsity of genomic data. This is a common challenge in personalized medicine research, as genomic data often has many variables but few samples.
    \item Integrating additional data types (e.g., EEG, imaging) to improve model performance. Multimodal data integration is essential for a holistic view of the patient but increases complexity.
    \item Generalizing models to other anti-epileptic drugs. This is crucial for wider clinical applicability in personalized epilepsy treatment.
    \item Collaborating with regulatory bodies for clinical adoption. AI-driven personalized medicine tools require rigorous validation and regulatory approval for safe and effective use.
    \item Increasing dataset size to enhance model performance (targeting $\sim$350 patients for AUC = 0.9). Larger datasets are vital for building robust and generalizable predictive models in personalized healthcare.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item The sample size, while sufficient for the study, could be larger to further enhance model performance and generalizability. 
    \item The complexity of the GBDT model, while providing good predictive power, makes it difficult to interpret the specific contributions of individual features.
    
\end{itemize}

% ============================== ARTICLE 2 =====================================
\section{Diabetes Prediction Using Machine Learning and Explainable AI Techniques}
\textbf{Article Reference:} \cite{article_2}

\subsection*{Overview}
This study proposes an automated diabetes prediction system using ML and explainable AI. The system combines the public Pima Indian dataset with a private dataset collected from female workers in a Bangladeshi textile factory. The system addresses data imbalance, missing values, and is deployed for real-time prediction via web and mobile applications. The development of non-invasive AI-driven tools for diabetes detection, as presented in this paper, contributes to personalized healthcare by enabling earlier and more accessible diagnosis.

\subsection*{Dataset}
\begin{itemize}
    \item \textbf{Pima Indian Dataset:} 768 records, 268 diabetes-positive; includes 8 features.
    \item \textbf{RTML Private Dataset:} 203 female employees; features similar to Pima dataset but lacks insulin values.
\end{itemize}

\subsection*{Data Processing}
\begin{itemize}
    \item Zero values in the merged dataset were replaced with corresponding mean values and the dataset was separated into training and test sets using the holdout validation technique.
    \item Mutual information was used to measure the interdependence of variables and feature importance.
    \item A semi-supervised approach using the extreme gradient boosting technique (XGB regressor) was used to predict the missing insulin feature of the RTML dataset.
\end{itemize}

\subsection*{Approach}
Various models were tested: decision trees, KNN, SVM, random forest, logistic regression, AdaBoost, XGBoost, bagging, and voting classifiers. Hyperparameters were tuned using GridSearchCV. The final model employed XGBoost with ADASYN for balancing. The choice of XGBoost is appropriate due to its effectiveness in handling complex datasets, but the lack of inherent explainability highlights the need for methods.

\subsection*{Results}
\begin{itemize}
    \item Accuracy: 81\%
    \item F1 Score: 0.81
    \item AUC: 0.84
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item Missing insulin values required imputation via semi-supervised learning. This introduces a degree of uncertainty into the model.
    \item Class imbalance necessitated oversampling (SMOTE, ADASYN). Oversampling techniques can sometimes lead to overfitting.
    \item Limited private dataset size may hinder generalizability. Larger, more diverse datasets would improve the robustness of the model.
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Expanding dataset size for better robustness.
    \item Integrating fuzzy logic and optimization for improved prediction.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item The use of imputation for missing insulin values introduces some uncertainty.    
    \item The private dataset is relatively small, which may limit the model's generalizability.    
\end{itemize}

% ============================== ARTICLE 3 =====================================
\section{Integrating Machine Learning and Deep Learning Techniques for Advanced Alzheimer's Disease Detection through Gait Analysis}
\textbf{Article Reference:} \cite{article_3}

\subsection*{Overview}
The paper aims to enhance early detection of Alzheimer's Disease (AD) by leveraging gait analysis combined with advanced machine learning (ML) and deep learning (DL) techniques. Gait abnormalities, such as reduced stride length and irregular cadence, are identified as early biomarkers for cognitive decline associated with AD. The study emphasizes the need for non-invasive, scalable diagnostic tools. This research highlights the potential of AI-driven gait analysis to contribute to personalized AD management through early detection.

\subsection*{Dataset}
Data were collected using wearable sensors and motion capture systems in both clinical and real-world environments, providing high-resolution temporal and spatial gait metrics. The dataset includes gait features like stride length, cadence, swing time, and gait variability, with some data sourced from publicly available repositories like the UCI Machine Learning Repository.

\subsection*{Data Processing}
\begin{itemize}
    \item \textbf{Normalization:} Features were scaled between 0 and 1 to standardize the data, ensuring that features with larger ranges (e.g., stride length) did not dominate the model training.
    \item \textbf{Handling Missing Data:} Missing values were imputed using median substitution to maintain data integrity and reduce bias.
    \item \textbf{Class Imbalance:} The Synthetic Minority Over-sampling Technique (SMOTE) was applied to generate synthetic samples of the minority class (AD patients), addressing class imbalance issues.
    \item \textbf{Feature Selection:} Recursive Feature Elimination (RFE) was used to identify the most significant gait features—such as stride length, gait variability, and cadence—to improve model performance.
    \item \textbf{Correlation Analysis:} High correlations between key features (e.g., stride length and step length) validated their importance for prediction, informing feature selection.
\end{itemize}

\subsection*{Approach}
The study employed a hybrid deep learning model comprising Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to classify individuals as healthy or at risk for AD. These models analyzed temporal-spatial gait features, capturing sequential patterns and irregularities. Traditional ML classifiers such as Random Forest and SVM were also evaluated for comparison. The use of a hybrid CNN-RNN model is a strength, as it leverages the capabilities of both CNNs for spatial feature extraction and RNNs for temporal sequence modeling, which is well-suited for gait analysis.

\subsection*{Results}
\begin{itemize}
    \item Hybrid CNN-RNN model accuracy: 93\%
    \item Precision: 92\% 
    \item Recall: 91\%
    \item F1-score: 91.5\%
    \item AUC-ROC: 95\%
    \item Traditional models: Random Forest (88\%) and SVM (86\%)
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item The reliance on controlled datasets, which may not fully reflect real-world variability, impacting model robustness.
    \item The complexity and interpretability of deep learning models, posing a barrier for clinical acceptance.
    \item The need for large, diverse datasets to ensure generalizability.
    \item Integration into clinical workflows and validation through real-world testing.
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Incorporating multimodal data sources, such as MRI, PET scans, vocal, and cognitive measures, to improve diagnostic precision.
    \item Expanding datasets to include diverse populations and environmental conditions, enhancing model robustness.
    \item Developing explainable AI frameworks to improve interpretability and clinician trust.
    \item Extending studies to include longitudinal gait data for monitoring disease progression and enabling earlier detection.
    \item Conducting clinical pilot studies and developing affordable wearable technologies for widespread, low-resource application.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item The dataset may not fully represent the variability of real-world gait patterns.    
    \item Deep learning models are often considered "black boxes," which can hinder clinical acceptance.
\end{itemize}

% ============================== ARTICLE 4 =====================================
\section{Diabetes Detection Using Deep Learning Algorithms}
\textbf{Article Reference:} \cite{article_4}

\subsection*{Overview}
The authors developed a non-invasive method to detect diabetes using heart rate variability (HRV) signals derived from ECG data. They designed a deep learning architecture combining convolutional neural networks (CNN) and long short-term memory (LSTM) networks to automatically extract complex features from the HRV signals. These features were then classified using a support vector machine (SVM) with an RBF kernel. The approach achieved a high accuracy of 95.7\%, outperforming previous methods. This research demonstrates the potential of AI for non-invasive, personalized diabetes screening.

\subsection*{Dataset}
\begin{itemize}
    \item ECG recordings from 20 individuals (both diabetic and normal)
    \item Each participant provided a 10-minute ECG sample, from which heart rate time series data was derived
    \item Total datasets: 71 datasets for both groups, each containing 1000 samples
\end{itemize}

\subsection*{Data Processing}
\begin{itemize}
    \item Used Pan and Tompkins algorithm for QRS complex detection to extract heart rate intervals.
    \item Derived HRV signals directly from ECG without additional preprocessing.
    \item Input data fed into deep learning architectures for automatic feature learning.
\end{itemize}

\subsection*{Approach}
\begin{itemize}
    \item Built a deep learning model comprising 5 CNN layers followed by an LSTM layer to capture spatial and temporal features.
    \item Used dropout (0.1) for regularization.
    \item Extracted features automatically within the network, then classified using an SVM with RBF kernel.
    \item Employed 5-fold cross-validation for robust evaluation.
\end{itemize}
\subsection*{Results}
\begin{itemize}
    \item Maximum classification accuracy: 95.7\% (CNN-LSTM with SVM)
    \item Various architectures tested with accuracies ranging from 68\% to 95.7\%
    \item The combination of deep learning feature extraction with SVM classification outperformed using deep learning alone
    \item Highest accuracy reported so far for non-invasive diabetes detection using HRV signals
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item Limited dataset size could affect generalization; larger datasets are needed.
    \item Variability in HRV signals due to individual differences may pose challenges.
    \item Ensuring model interpretability for clinical acceptance.
    \item Moving from controlled datasets to real-world, noisy ECG signals.
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Increase dataset size to improve model accuracy and robustness.
    \item Explore anomaly prediction techniques by analyzing dynamic characteristics in HRV data.
    \item Develop more advanced deep learning models for early and accurate detection.
    \item Investigate applicability to real-time monitoring and broader clinical validation.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item The dataset size is limited (only 20 individuals), which may affect the model's ability to generalize to larger populations
    \item Like other deep learning models, the interpretability of the model could be a concern for clinical use
\end{itemize}


% ============================== ARTICLE 6 =====================================
\section{CardioXNet: A Novel Lightweight Deep Learning Framework for Cardiovascular Disease Classification Using Heart Sound Recordings}
\textbf{Article Reference:} \cite{article_6}

\subsection*{Overview}
This paper introduces CardioXNet, a lightweight CRNN architecture designed for the automatic detection of five types of heart sounds using raw PCG signals. The architecture involves two main phases: representation learning to extract time-invariant features and sequence residual learning to extract temporal features. CardioXNet is designed to be efficient for use in low-resource settings.

\subsection*{Dataset}
\begin{itemize}
    \item \textbf{Primary Dataset:} GitHub PCG database containing 1000 recordings across five classes: Normal, Aortic stenosis, Mitral regurgitation, Mitral stenosis, and Mitral valve prolapse.
    \item \textbf{Secondary Dataset:} PhysioNet/CinC 2016 challenge dataset with 3240 recordings labeled as normal or abnormal, used to test the model's generalizability.
\end{itemize}

\subsection*{Data Processing}
Normalisation

\subsection*{Approach}
The authors developed CardioXNet, a lightweight CRNN framework with two learning schemes:
\begin{itemize}
    \item \textbf{Representation learning:} Extracts time-invariant features using three parallel CNN pathways
    \item \textbf{Sequence residual learning:} Extracts temporal features using bidirectional connections
    \item The model is specifically designed to be efficient in terms of parameters and computational requirements
\end{itemize}

\subsection*{Results}
\begin{itemize}
    \item Achieved 99.60\% accuracy on the GitHub dataset, outperforming prior methods.
    \item Demonstrated 86.57\% accuracy on the PhysioNet dataset, indicating good generalization.
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item Limited dataset size, especially for specific conditions like HVD
    \item Lack of patient independence and demographic details
    \item Generalization to real-world, heterogeneous data remains untested
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Incorporate larger and more diverse PCG datasets
    \item Integrate CardioXNet into wearable devices with cloud connectivity
    \item Explore transfer learning and further model compression for resource-constrained deployment
    \item Develop methods to handle noisy recordings and variable acoustic environments
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item Dataset size and diversity might limit generalizability to broader populations
    \item Missing demographic and variability information limits assessment of performance across different patient groups
    \item Robustness to real-world noise and device variation not fully evaluated
    \item Limited evaluation in actual clinical settings
\end{itemize}

% % ============================== ARTICLE 8 =====================================
% \section{Cardiovascular Diseases Prediction by Machine Learning Incorporation with Deep Learning}
% \textbf{Article Reference:} \cite{article_8}

% \subsection*{Overview}
% The article discusses the increasing role of artificial intelligence (AI) in healthcare, particularly in predicting cardiovascular diseases (CVD). It highlights the growing prevalence of data from the Internet of Things (IoT) within healthcare systems, which can be leveraged to identify risk factors associated with CVD.

% \subsection*{Dataset}
% \begin{itemize}
%     \item Heart dataset with 918 unique samples after removing duplicates
%     \item Dataset comprises 11 features relevant for predicting CVD
% \end{itemize}

% \subsection*{Data Processing}
% The study employed machine learning models to analyze the data received from Internet of Things (IoT) devices. Traditional machine learning algorithms were noted to have limitations in accuracy, which led to the exploration of advanced techniques for better predictions. A feature selection method using Tree SHAP was applied to identify significant features influencing CVD predictions. This method enhances the interpretability of the results by showing each feature's contribution to predictions.

% \subsection*{Approach}
% The researchers proposed a stacking fusion model-based classifier, which achieved an impressive accuracy of nearly 96\%. This model effectively combined the strengths of various algorithms, outperforming individual models in predicting high-risk individuals for CVD. They emphasized the limitations of traditional machine learning and the importance of large, diverse datasets for robust predictions.

% \subsection*{Results}
% The proposed stacking fusion model-based classifier demonstrated superior performance, achieving nearly 96\% accuracy. The model's performance remained stable after feature selection, with AUC values not significantly impacted by the removal of the Resting ECG feature.

% \subsection*{Challenges}
% One of the challenges noted in the study is the reliance on traditional machine learning algorithms, which may not adequately account for data variability, leading to lower accuracy in predictions. Additionally, the need for more extensive datasets from various medical institutions was emphasized.

% \subsection*{Future Directions}
% \begin{itemize}
%     \item Incorporate additional deep learning techniques within IoT environments
%     \item Enhance accuracy for identifying high-risk patients
%     \item Implement early clinical interventions based on model predictions
%     \item Develop more sophisticated models that can handle real-time streaming data
% \end{itemize}

% \subsection*{Critique}
% \begin{itemize}
%     \item Need for broader dataset diversity incorporating various demographics and geographical locations
%     \item More detailed explanation of the specific algorithms used in the stacking model would clarify their individual contributions
%     \item Longitudinal studies would help assess the real-world effectiveness of the models
%     \item The paper does not adequately address how data imbalance was handled
% \end{itemize}

% ============================== ARTICLE 9 =====================================
\section{Optimizing Type 2 Diabetes Management: AI-Enhanced Time Series Analysis of Continuous Glucose Monitoring Data for Personalized Dietary Intervention}
\textbf{Article Reference:} \cite{article_9}

\subsection*{Overview}
This study proposes a method to optimize type 2 diabetes management using AI-enhanced time series analysis of continuous glucose monitoring (CGM) data. The goal is to enable personalized dietary interventions to improve patient outcomes.

\subsection*{Dataset}
\begin{itemize}
    \item Collected CGM data from 8 patients with type 2 diabetes.
    \item Data includes time-series blood glucose (BG) values.
\end{itemize}

\subsection*{Data Processing}
\begin{itemize}
    \item Removed NaN records to clean the data.
    \item Applied feature extraction and dimensionality reduction techniques.
    \item Split dataset into training (75\%) and testing (25\%) portions.
\end{itemize}

\subsection*{Approach}
\begin{itemize}
    \item Used regression models: XGBoost, SARIMA, Prophet.
    \item Integrated dietary recommendations based on predicted BG levels.
\end{itemize}

\subsection*{Results}
\begin{itemize}
    \item XGBoost outperformed SARIMA and Prophet.
    \item Achieved high R² and low MAPE.
    \item Accurately predicted glucose fluctuations for timely intervention.
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item Limited dataset (only 8 patients) reduced model generalizability.
    \item Up to ±30-minute lag in CGM readings affected prediction accuracy.
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Expand dataset to improve training and validation.
    \item Enhance model with additional features and contextual data.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item Promising approach, but small sample limits robustness.
    \item Time lag in CGM data must be addressed for better accuracy.
\end{itemize}



% ============================== ARTICLE 20 =====================================
\section{AI-Based COVID-19 Detection Using CT Imaging}
\textbf{Article Reference:} \cite{article_20}

\subsection*{Overview}
The article presents a deep learning algorithm developed to screen for COVID-19 using CT images, motivated by the limitations of traditional pathogenic testing methods (e.g., nucleic acid testing), which may yield false negatives. The main objective is to provide rapid and accurate diagnoses to help mitigate the spread of the virus.

\subsection*{Data Used}
\begin{itemize}
    \item A total of 1,065 CT images were collected.
    \item Included 180 cases of typical viral pneumonia and 79 confirmed COVID-19 cases from three hospitals.
    \item An additional 15 COVID-19 cases were added where initial nucleic acid tests were negative.
\end{itemize}

\subsection*{Data Preprocessing}
\begin{itemize}
    \item All images were resized to 299 × 299 pixels for input consistency.
    \item Lung regions were manually delineated to focus on regions of interest (ROIs), improving model focus.
    \item Images were converted to a virtual RGB format to match the input requirements of the modified inception model.
\end{itemize}

\subsection*{Approach Used}
\begin{itemize}
    \item A transfer learning strategy was employed using a modified inception model, referred to as M-inception.
    \item Only the customized layers were trained, leveraging pre-trained weights to reduce overfitting and training time.
    \item The architecture aimed to classify CT images to distinguish between COVID-19 and other viral pneumonias.
\end{itemize}

\subsection*{Results}
\begin{itemize}
    \item Internal validation accuracy was 89.5\% (Specificity: 0.88, Sensitivity: 0.87).
    \item External test accuracy was 79.3\% (Specificity: 0.83, Sensitivity: 0.67).
    \item In 54 cases where nucleic acid tests were initially negative, the model correctly predicted 46 cases, achieving an accuracy of 85.2\%.
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item CT images often include irrelevant elements, complicating automated diagnosis.
    \item A relatively small training dataset may reduce generalizability and increase model bias.
    \item Low signal-to-noise ratio and data heterogeneity pose significant hurdles in applying deep learning in clinical diagnostics.
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Expand the dataset to cover a larger number of COVID-19 cases across various pathological stages.
    \item Optimize the model's robustness, accuracy, and reliability by including diverse imaging scenarios.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item The study provides a compelling proof-of-concept for AI-based diagnostics, but its effectiveness is limited by dataset size.
    \item Future work should emphasize external validation and distinguishability from other respiratory diseases.
\end{itemize}



% ============================== ARTICLE 21 =====================================
\section{Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning}
\textbf{Article Reference:} \cite{article_21}

\subsection*{Overview}
This study presents the development of an artificial intelligence (AI) system based on transfer learning to diagnose medical conditions using medical imaging. The authors focus on the classification of images for macular degeneration, diabetic retinopathy, and pneumonia from chest X-rays. The aim is to demonstrate that AI can improve diagnostic accuracy and speed, often matching or exceeding human performance.

\subsection*{Data Used}
\begin{itemize}
    \item \textbf{Optical Coherence Tomography (OCT)}: Out of 207,130 collected OCT images, 108,312 high-quality images were retained after a quality review for training.
    \item \textbf{Chest X-ray Images}: A dataset consisting of 5,232 pediatric chest X-rays was used to train the model for pneumonia classification, with an additional 624 patient images used for testing.
\end{itemize}

\subsection*{Data Preprocessing}
\begin{itemize}
    \item Quality control to filter OCT images.
    \item Categorization of images according to medical diagnosis to facilitate supervised learning.
\end{itemize}

\subsection*{Approach}
\begin{itemize}
    \item The model utilizes a transfer learning strategy, initializing from pre-trained convolutional neural networks to boost performance despite the relatively small dataset size.
    \item Classification tasks included urgent referral detection for conditions like choroidal neovascularization and diabetic macular edema.
    \item The approach also focused on distinguishing between bacterial and viral pneumonia on pediatric chest X-rays.
\end{itemize}

\subsection*{Results}
\begin{itemize}
    \item \textbf{96.6\% accuracy} in OCT image classification.
    \item \textbf{92.8\% accuracy} in pneumonia detection from chest X-rays.
    \item Performance was comparable to that of experienced clinicians, highlighting the potential of AI in supporting medical decision-making.
\end{itemize}

\subsection*{Challenges}
\begin{itemize}
    \item Difficulty in obtaining large and diverse labeled medical datasets.
    \item Clinical interpretability of the AI decisions, a common barrier in the adoption of deep learning in healthcare.
\end{itemize}

\subsection*{Future Directions}
\begin{itemize}
    \item Expanding the dataset with images from multiple imaging devices and settings to improve model generalizability.
    \item Extending the transfer learning approach to other imaging modalities beyond OCT and X-ray.
\end{itemize}

\subsection*{Critique}
\begin{itemize}
    \item While the approach is promising and the results impressive, the generalizability of the model remains a concern due to dataset limitations.
    \item Further validation with larger, more heterogeneous datasets is necessary.
    \item The open release of data and code is a notable contribution, encouraging reproducibility and further research in AI for medical imaging.
\end{itemize}

% \newpage
% \section{Discussion}
% As presented in the table above, different methods were proposed using machine learning which are seen as the most effective technique to build an intrusion detection system and detect attacks with different datasets mostly NSL-KDD dataset which resulted in a good accuracy.\\
% Nearly half of the studies did not work with normalization, there are also studies that did not perform a feature selection, as well as the most did not test many learning algorithms and the most of studies work on binary classification (normal or attack).


% \section{Conclusion}
% In this chapter, we studied and evaluated the latest methods proposed by recent research, about the use of machine learning methods for attacks classification based on data-sets.

% Most of the studied architectures could more or less achieve satisfying results with high accuracy. However, further research using large and fresh data-sets is always required to improve the performance of the existing methods for more efficient results.



